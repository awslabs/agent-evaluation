{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Agent Evaluation is a generative AI-powered framework for testing virtual agents.</p> <p>Internally, Agent Evaluation implements an LLM agent (evaluator) that will orchestrate conversations with your own agent (target) and evaluate the responses during the conversation.</p>"},{"location":"#key-features","title":"Key features","text":"<p>\u2705 Evaluate an agent's responses by simulating concurrent, multi-turn conversations.</p> <p>\u2705 Built-in support for popular AWS services including Amazon Bedrock, Amazon Q Business, and Amazon SageMaker. You can also bring your own agent to test using Agent Evaluation.</p> <p>\u2705 Define hooks to perform additional tasks such as integration testing.</p> <p>\u2705 Incorporate into CI/CD pipelines to expedite the time to delivery while maintaining the stability of agents in production environments.</p> <ul> <li> <p>\ud83d\ude80 Getting started</p> <p>Create your first test using Agent Evaluation.</p> <p> User Guide</p> </li> <li> <p>\ud83c\udfaf Built-in targets</p> <p>View the required configurations for your agent.</p> <p> Targets</p> </li> <li> <p>\u270f\ufe0f Writing test cases</p> <p>Learn how to write test cases in Agent Evaluation.</p> <p> User Guide</p> </li> <li> <p> Contribute</p> <p>Review the contributing guidelines to get started!</p> <p> GitHub</p> </li> </ul>"},{"location":"cli/","title":"agenteval","text":"<p>Usage:</p> <pre><code>agenteval [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#agenteval-init","title":"agenteval init","text":"<p>Initialize a test plan.</p> <p>Usage:</p> <pre><code>agenteval init [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --plan-dir TEXT  The directory to store the test plan. If a directory is not\n                   provided, the test plan will be saved to the current\n                   working directory.\n  --help           Show this message and exit.\n</code></pre>"},{"location":"cli/#agenteval-run","title":"agenteval run","text":"<p>Run test plan.</p> <p>Usage:</p> <pre><code>agenteval run [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --filter TEXT          Specifies the test(s) to run, where multiple tests\n                         should be seperated using a comma. If a filter is not\n                         provided, all tests will be run.\n  --plan-dir TEXT        The directory where the test plan is stored. If a\n                         directory is not provided, the test plan will be read\n                         from the current working directory.\n  --verbose              Whether to enable verbose logging. Defaults to False.\n  --num-threads INTEGER  Number of threads used to run tests concurrently. If\n                         the number of threads is not provided, the thread\n                         count will be set to the number of tests (up to a\n                         maximum of 45 threads).\n  --work-dir TEXT        The directory where the test result and trace will be\n                         generated. If a directory is not provided, the assets\n                         will be saved to the current working directory.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"agenteval.yml<pre><code>evaluator:\n  model: claude-3\ntarget:\n  type: bedrock-agent\n  bedrock_agent_id: string\n  bedrock_agent_alias_id: string\ntests:\n  retrieve_missing_documents:\n    steps:\n    - Ask agent for a list of missing documents for claim-006.\n    expected_results:\n    - The agent returns a list of missing documents.\n    initial_prompt: Give me a list of missing documents for claim-006.\n    max_turns: 2\n    hook: path.to.MyHook\n</code></pre> <p><code>evaluator</code> (map)</p> <p>Refer to Evaluators for the available configurations.</p> <p><code>target</code> (map)</p> <p>Refer to Targets for the available configurations.</p> <p><code>tests</code> (map)</p> <p>A map of test cases, where the test name serves as the key.</p> <p><code>steps</code> (list of strings)</p> <p>The steps to perform for the test.</p> <p><code>expected_results</code> (list of strings)</p> <p>The expected results for the test.</p> <p><code>initial_prompt</code> (string; optional)</p> <p>The first message that is sent to the agent, which starts the conversation. If unspecified, the message will be generated based on the <code>steps</code> provided.</p> <p><code>max_turns</code> (integer; optional)</p> <p>The maximum number of user-agent exchanges before the test fails. The default is <code>2</code>.</p> <p><code>hook</code> (string; optional)</p> <p>The module path to an evaluation hook. Refer to Hooks for more details.</p>"},{"location":"hooks/","title":"Hooks","text":"<p>You can specify hooks that run before and/or after evaluating a test. This is useful for performing integration testing, as well as any setup or cleanup tasks required.</p> <p>To create your hooks, define a Python module containing a subclass of Hook. The name of this module must contain the suffix <code>_hook</code> (e.g. <code>my_evaluation_hook</code>).</p> <ul> <li> <p>Implement the <code>pre_evaluate</code> method for a hook that runs before evaluation. In this method, you have access to the Test and Trace via the <code>test</code> and <code>trace</code> arguments, respectively.</p> </li> <li> <p>Implement the <code>post_evaluate</code> method for a hook that runs after evaluation,. Similar to the <code>pre_evaluate</code> method, you have access to the Test and Trace. You also have access to the TestResult via the <code>test_result</code> argument. You may override the attributes of the <code>TestResult</code> if you plan to use this hook to perform additional testing, such as integration testing.</p> </li> </ul> my_evaluation_hook.py<pre><code>from agenteval import Hook\n# import dependencies here\n\nclass MyEvaluationHook(Hook):\n\n    def pre_evaluate(test, trace):\n    # implement logic here\n\n    def post_evaluate(test, test_result, trace):\n    # implement logic here\n</code></pre> <p>Once you have created your subclass, specify the module path to the hook.</p> agenteval.yml<pre><code>tests:\n  make_reservation:\n    hook: my_evaluation_hook.MyEvaluationHook\n</code></pre>"},{"location":"hooks/#examples","title":"Examples","text":""},{"location":"hooks/#integration-testing-using-post_evaluate","title":"Integration testing using <code>post_evaluate</code>","text":"<p>In this example, we will test an agent that can make dinner reservations. In addition to evaluating the conversation, we want to test that the reservation is written to the backend database. To do this, we will create a post evaluation hook that queries a PostgreSQL database for the reservation record. If the record is not found, we will override the <code>TestResult</code>.</p> <p>test_record_insert_hook.py</p> <pre><code>import boto3\nimport json\nimport psycopg2\n\nfrom agenteval import Hook\n\nSECRET_NAME = \"test-secret\"\n\ndef get_db_secret() -&gt; dict:\n\n  session = boto3.session.Session()\n  client = session.client(\n      service_name='secretsmanager',\n  )\n  get_secret_value_response = client.get_secret_value(\n      SecretId=SECRET_NAME\n  )\n  secret = get_secret_value_response['SecretString']\n\n  return json.loads(secret)\n\nclass TestRecordInsertHook(Hook):\n\n  def post_evaluate(test, test_result, trace):\n\n    # get database secret from AWS Secrets Manager\n    secret = get_db_secret()\n\n    # connect to database\n    conn = psycopg2.connect(\n      database=secret[\"dbname\"],\n      user=secret[\"username\"],\n      password=secret[\"password\"],\n      host=secret[\"host\"],\n      port=secret[\"port\"]\n    )\n\n    # check if record is written to database\n    with conn.cursor() as cur:\n      cur.execute(\"SELECT * FROM reservations WHERE name = 'Bob'\")\n      row = cur.fetchone()\n\n    # override the test result based on query result \n    if not row:\n      test_result.passed = False\n      test_result.result = \"Integration test failed\"\n      test_result.reasoning = \"Record was not inserted into the database\"\n</code></pre> <p>Create a test that references the hook.</p> <p>agenteval.yml</p> <pre><code>tests:\n  make_reservation:\n    steps:\n    - Ask agent to make a reservation under the name Bob for 7 PM.\n    expected_results:\n    - The agent confirms that a reservation has been made.\n    hook: test_record_insert_hook.TestRecordInsert\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Agent Evaluation requires <code>python&gt;=3.9</code>. Please make sure you have an acceptable version of Python before proceeding.</p> <p>To install:</p> <pre><code>pip install agent-evaluation\n</code></pre> <p>You can also install from source by cloning the repository and installing from the project root.</p> <pre><code>pip install .\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#getting-started","title":"Getting started","text":"<p>To begin, initialize a test plan.</p> <pre><code>agenteval init\n</code></pre> <p>This will create a configuration file named <code>agenteval.yml</code> in the current directory.</p> agenteval.yml<pre><code>evaluator:\n  model: claude-3\ntarget:\n  type: bedrock-agent\n  bedrock_agent_id: null\n  bedrock_agent_alias_id: null\ntests:\n  retrieve_missing_documents:\n    steps:\n    - Ask agent for a list of missing documents for claim-006.\n    expected_results:\n    - The agent returns a list of missing documents.\n</code></pre> <p>If you are testing an Amazon Bedrock agent, update the following <code>target</code> configurations:</p> <ul> <li><code>bedrock_agent_id</code>: The unique identifier of the Amazon Bedrock agent.</li> <li><code>bedrock_agent_alias_id</code>: The alias of the Amazon Bedrock agent.</li> </ul> <p>Note</p> <p>Refer to Targets for additional configurations.</p> <p>Update <code>tests</code> with your test cases. Each test must have the following:</p> <ul> <li><code>steps</code>: A list of steps you want to perform in your test.</li> <li><code>expected_results</code>: A list of expected results for your test.</li> </ul> <p>Once you have updated the test plan, you can run your tests:</p> <p>Warning</p> <p>The default evaluator is powered by Anthropic's Claude 3 Sonnet model on Amazon Bedrock. The charges you incur from using Amazon Bedrock will be your responsibility. Please review this page on evaluator costs before running your tests.</p> <pre><code>agenteval run\n</code></pre> <p>The results will be printed in your terminal and a Markdown summary will be available in <code>agenteval_summary.md</code>.</p> <p>You will also find traces saved under <code>agenteval_traces/</code>. This is useful for understanding the flow of evaluation.</p>"},{"location":"user_guide/#writing-test-cases","title":"Writing test cases","text":"<p>It is important to be clear and concise when writing your test cases.</p> agenteval.yml<pre><code>tests:\n  get_open_claims:\n    steps:\n    - Ask the agent which claims are open.\n    expected_results:\n    - The agent returns a list of open claims.\n</code></pre> <p>If your test case is complex, consider breaking it down into multiple, smaller <code>tests</code>.</p>"},{"location":"user_guide/#multi-turn-conversations","title":"Multi-turn conversations","text":"<p>To test multiple user-agent interactions, you can provide multiple <code>steps</code> to orchestrate the interaction.</p> agenteval.yml<pre><code>tests:\n  get_open_claims_with_details:\n    steps:\n    - Ask the agent which claims are open.\n    - Ask the agent for details on claim-006.\n    expected_results:\n    - The agent returns a list of open claims.\n    - The agent returns the details on claim-006.\n</code></pre> <p>The maximum number of turns allowed for a conversation is configured using the <code>max_turns</code> parameter for the test (defaults to <code>2</code> when not specified). If the number of turns in the conversation reaches the <code>max_turns</code> limit, then the test will fail.</p>"},{"location":"user_guide/#providing-data","title":"Providing data","text":"<p>You can test an agent's ability to prompt the user for data when you include it within the step. For example:</p> agenteval.yml<pre><code>tests:\n  get_auto_open_claims:\n    steps:\n    - Ask the agent which claims are open.\n      When the agent asks for the claim type, respond with \"Auto\".\n    expected_results:\n    - The agent returns claim-001 and claim-002\n</code></pre>"},{"location":"user_guide/#specify-the-first-user-message","title":"Specify the first user message","text":"<p>By default, the first user message in the test is automatically generated based on the first step. To override this message, you can specify the <code>initial_prompt</code>.</p> agenteval.yml<pre><code>tests:\n  get_claims_with_missing_documents:\n    steps:\n    - Ask agent which claims still have missing documents.\n    initial_prompt: Can you let me know which claims still have missing documents?\n    expected_results:\n    - The agent returns claim-003 and claim-004\n</code></pre>"},{"location":"cicd/","title":"Integration with CI/CD Pipelines","text":"<p>After validating the functionality in the development account, you can commit the code to the repository and initiate the deployment process for the virtual agent to the next stage. Seamless integration with CI/CD pipelines is a crucial aspect of Agent Evaluation, enabling comprehensive integration testing to ensure that no regressions are introduced during new feature development or updates. This rigorous testing approach is vital for maintaining the reliability and consistency of virtual agents as they progress through the software delivery lifecycle.</p> <p>By incorporating Agent Evaluation into CI/CD workflows, organizations can automate the testing process, ensuring that every code change or update undergoes thorough evaluation before deployment. This proactive measure minimizes the risk of introducing bugs or inconsistencies that could compromise the virtual agent's performance and the overall user experience.</p>"},{"location":"cicd/#cicd-workflow","title":"CI/CD workflow","text":"<p>The figure below shows what a standard agent CI/CD pipeline looks like:</p> <p></p> <ol> <li>The source repository stores the agent configuration, including agent instructions, system prompts, model configuration, etc. You should always commit your changes to ensure quality and reproducibility.</li> <li>When you commit your changes, a build step is triggered. This is where unit tests should run and validate the changes, including typo and syntax checks.</li> <li>When the changes are deployed to the staging environment, Agent Evaluation should run with a series of test cases for runtime validation.</li> <li>The runtime validation on the staging environment will help build confidence to deploy the fully tested agent to production.</li> </ol>"},{"location":"cicd/#step-by-step-github-actions-setup","title":"Step-by-step GitHub Actions setup","text":"<p>We have built an example with GitHub Actions, please take a look at the Github workflow. Here is the step-by-step setup guide:</p> <ol> <li> <p>Write a series of test cases following the agent-evaluation test plan syntax. Store test plans in the git repository. For example, a test plan to test a Bedrock agent target is written as follows, with <code>BEDROCK_AGENT_ALIAS_ID</code> and <code>BEDROCK_AGENT_ID</code> as placeholders:</p> <pre><code>evaluator:\nmodel: claude-3\ntarget:\n    bedrock_agent_alias_id: BEDROCK_AGENT_ALIAS_ID\n    bedrock_agent_id: BEDROCK_AGENT_ID\ntype: bedrock-agent\ntests:\n    InsuranceClaimQuestions:\n    ...\n</code></pre> </li> <li> <p>Create an IAM user with proper permissions:</p> <ol> <li>The principal must have <code>InvokeModel</code> permission to the model specified in the configuration.</li> <li>The principal must have the permissions to call the target agent. Depending on the target type, different permissions are required. Please visit the agent-evaluation Target docs for details.</li> </ol> </li> <li>Store the IAM credentials (<code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>  ) in GitHub Actions secrets. </li> <li>Configure a GitHub workflow  as follows:     <pre><code>name: CI/CD example\n\non:\npush:\n    branches: [ \"main\" ]\n\nenv:\nAWS_REGION: us-east-1                   # set this to your preferred AWS region, e.g. us-west-1\n\n\npermissions:\ncontents: read\n\njobs:\ndeploy:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout\n    uses: actions/checkout@v4\n\n    - name: Configure AWS credentials\n    uses: aws-actions/configure-aws-credentials@v4\n    with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: ${{ env.AWS_REGION }}\n\n    - name: Install agent-evaluation\n    run: |\n        pip install agent-evaluation\n        agenteval --help\n    - name: Test Bedrock Agent\n    id: test-bedrock-agent\n    env:\n        BEDROCK_AGENT_ALIAS_ID: ${{ vars.BEDROCK_AGENT_ALIAS_ID }}\n        BEDROCK_AGENT_ID: ${{ vars.BEDROCK_AGENT_ID }}\n    run: |\n        sed -e \"s/BEDROCK_AGENT_ALIAS_ID/$BEDROCK_AGENT_ALIAS_ID/g\" -e \"s/BEDROCK_AGENT_ID/$BEDROCK_AGENT_ID/g\" &lt;path-to-the-test-plan-template-file&gt; &gt; agenteval.yml\n        agenteval run\n    - name: Test Summary\n    if: always()\n    id: test-summary\n    run: |\n        if [ -f agenteval_summary.md ]; then\n        cat agenteval_summary.md &gt;&gt; $GITHUB_STEP_SUMMARY\n        fi\n</code></pre></li> </ol>"},{"location":"evaluators/","title":"Evaluators","text":"<p>An evaluator is a LLM agent that evaluates a Target on a test. Evaluators utilize foundation models directly on Amazon Bedrock. They do not make use of the Agents for Amazon Bedrock functionality.</p>"},{"location":"evaluators/#evaluation-workflow","title":"Evaluation workflow","text":"<p>The diagram below depicts the workflow that is conducted during evaluation.</p> <pre><code>graph TD\n  classDef nodeText font-size:10pt;\n  A((Start)) --&gt; B{Initial&lt;br&gt;prompt?}\n  B --&gt;|yes| C(Invoke agent)\n  B --&gt;|no| D(Generate initial prompt)\n  D --&gt; C\n  C --&gt; E(Get test status)\n  E --&gt; F{All steps&lt;br&gt;attempted?}  \n  F --&gt; |yes| G(Evaluate conversation)\n  F --&gt; |no| H{Max turns&lt;br&gt;reached?}\n  H --&gt; |yes| I(Fail)\n  H --&gt; |no| J(Generate user response)\n  J --&gt; C\n  G --&gt; K{All expected&lt;br&gt;results&lt;br&gt;observed?}\n  K --&gt; |yes| L(Pass)\n  K --&gt; |no| I(Fail)\n  I --&gt; M((End))\n  L --&gt; M\n  class A,B,C,D,E,F,G,H,I,J,K,L,M nodeText;\n  style I stroke:#f00\n  style L stroke:#0f0</code></pre>"},{"location":"evaluators/#evaluator-costs","title":"Evaluator costs","text":"<p>By default, evaluators will utilize the InvokeModel API with On-Demand mode, which will incur AWS charges based on input tokens processed and output tokens generated. You can find the latest pricing details for Amazon Bedrock here.</p> <p>The cost of running an evaluator for a single test is influenced by the following:</p> <ol> <li>The number and length of the steps.</li> <li>The number and length of expected results.</li> <li>The length of the target agent's responses.</li> </ol> <p>You can view the total number of input tokens processed and output tokens generated by the evaluator using <code>--verbose</code> flag when you perform a run (<code>agenteval run --verbose</code>).</p> <p>Note</p> <p>If you have purchased Provisioned Throughput model units for the <code>model</code> used to run evaluation, you can specify this resource using the <code>provisioned_throughput_arn</code> configuration.</p>"},{"location":"evaluators/#example","title":"Example","text":"<p>Let's use this Amazon Bedrock agent as a target we want to test.</p> <p>For the following test case:</p> agenteval.yml<pre><code>tests:\n  retrieve_missing_documents:\n    steps:\n    - Ask agent for a list of missing documents for claim-006.\n    expected_results:\n    - The agent returns a list of missing documents.\n</code></pre> <p>We find that on average, the evaluator processes ~583 input tokens and generates ~290 output tokens. </p>"},{"location":"evaluators/#prerequisites","title":"Prerequisites","text":"<p>The principal must have InvokeModel to the <code>model</code> specified in the configuration.</p>"},{"location":"evaluators/#configurations","title":"Configurations","text":"<p>Info</p> <p>This project uses Boto3's credential resolution chain to determine the AWS credentials to use. Please refer to the Boto3 documentation for more details.</p> agenteval.yml<pre><code>evaluator:\n  model: claude-3\n  provisioned_throughput_arn: my-throughput-arn\n  aws_profile: my-profile\n  aws_region: us-west-2\n  endpoint_url: my-endpoint-url\n  max_retry: 10\n</code></pre> <p><code>model</code> (string)</p> <p>Name of the model used to run evaluation. This must be one of:</p> <ul> <li><code>claude-3</code> (Claude 3 Sonnet)</li> <li><code>claude-3_5</code> (Claude 3.5 Sonnet)</li> <li><code>claude-3_7-us</code> (Claude 3.7 Sonnet)</li> <li><code>claude-haiku-3_5-us</code> (Claude 3.5 Haiku)</li> <li><code>llama-3_3-us</code> (Llama 3.3 70B)</li> </ul>"},{"location":"evaluators/#the-models-suffixed-with-us-are-using-default-usa-cross-region-inference-profile-bedrock-cross-region-documentation-link","title":"The models suffixed with <code>-us</code> are using default USA cross region inference profile. Bedrock cross region documentation link.","text":"<p><code>custom-config</code> (dict; optional)</p> <p>A valid combination with keys <code>model_id</code> and <code>request_body</code> specifying which foundation model with what configuration to invoke Bedrock. See Bedrock documentation or the default configurations in <code>src/agenteval/evaluators/model_config/preconfigured_model_configs.py</code>. Currently, only Meta and Anthropic models are supported.</p> <p><code>provisioned_throughput_arn</code> (string; optional)</p> <p>The Amazon Resource Name (ARN) of the Provisioned Throughput.</p> <p><code>aws_profile</code> (string; optional)</p> <p>A profile name that is used to create a Boto3 session.</p> <p><code>aws_region</code> (string; optional)</p> <p>The AWS region that is used to create a Boto3 session.</p> <p><code>endpoint_url</code> (string; optional)</p> <p>The endpoint URL for the AWS service which is used to construct the Boto3 client.</p> <p><code>max_retry</code> (integer; optional)</p> <p>Configures the Boto3 client with the maximum number of retry attempts allowed. The default is <code>10</code>.</p>"},{"location":"reference/base_target/","title":"BaseTarget","text":""},{"location":"reference/base_target/#src.agenteval.targets.base_target.BaseTarget","title":"<code>BaseTarget</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the common interface for target classes.</p> Source code in <code>src/agenteval/targets/base_target.py</code> <pre><code>class BaseTarget(ABC):\n    \"\"\"Defines the common interface for target classes.\"\"\"\n\n    @abstractmethod\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n        \"\"\"Invoke the target with a prompt.\n\n        Args:\n            prompt (str): The prompt as a string.\n\n        Returns:\n            TargetResponse\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/base_target/#src.agenteval.targets.base_target.BaseTarget.invoke","title":"<code>invoke(prompt)</code>  <code>abstractmethod</code>","text":"<p>Invoke the target with a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt as a string.</p> required <p>Returns:</p> Type Description <code>TargetResponse</code> <p>TargetResponse</p> Source code in <code>src/agenteval/targets/base_target.py</code> <pre><code>@abstractmethod\ndef invoke(self, prompt: str) -&gt; TargetResponse:\n    \"\"\"Invoke the target with a prompt.\n\n    Args:\n        prompt (str): The prompt as a string.\n\n    Returns:\n        TargetResponse\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/hook/","title":"Hook","text":""},{"location":"reference/hook/#src.agenteval.hook.Hook","title":"<code>Hook</code>","text":"<p>An evaluation hook.</p> Source code in <code>src/agenteval/hook.py</code> <pre><code>class Hook:\n    \"\"\"An evaluation hook.\"\"\"\n\n    def pre_evaluate(test: Test, trace: Trace) -&gt; None:\n        \"\"\"\n        Method called before evaluation. Can be used to perform any setup tasks.\n\n        Args:\n            test (Test): The test case.\n            trace (Trace): Captures steps during evaluation.\n        \"\"\"\n        pass\n\n    def post_evaluate(test: Test, test_result: TestResult, trace: Trace) -&gt; None:\n        \"\"\"\n        Method called after evaluation. This may be used to perform integration testing\n        or clean up tasks.\n\n        Args:\n            test (Test): The test case.\n            test_result (TestResult): The result of the test, which can be overriden\n                by updating the attributes of this object.\n            trace (Trace): Captures steps during evaluation.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/hook/#src.agenteval.hook.Hook.post_evaluate","title":"<code>post_evaluate(test, test_result, trace)</code>","text":"<p>Method called after evaluation. This may be used to perform integration testing or clean up tasks.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>Test</code> <p>The test case.</p> required <code>test_result</code> <code>TestResult</code> <p>The result of the test, which can be overriden by updating the attributes of this object.</p> required <code>trace</code> <code>Trace</code> <p>Captures steps during evaluation.</p> required Source code in <code>src/agenteval/hook.py</code> <pre><code>def post_evaluate(test: Test, test_result: TestResult, trace: Trace) -&gt; None:\n    \"\"\"\n    Method called after evaluation. This may be used to perform integration testing\n    or clean up tasks.\n\n    Args:\n        test (Test): The test case.\n        test_result (TestResult): The result of the test, which can be overriden\n            by updating the attributes of this object.\n        trace (Trace): Captures steps during evaluation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/hook/#src.agenteval.hook.Hook.pre_evaluate","title":"<code>pre_evaluate(test, trace)</code>","text":"<p>Method called before evaluation. Can be used to perform any setup tasks.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>Test</code> <p>The test case.</p> required <code>trace</code> <code>Trace</code> <p>Captures steps during evaluation.</p> required Source code in <code>src/agenteval/hook.py</code> <pre><code>def pre_evaluate(test: Test, trace: Trace) -&gt; None:\n    \"\"\"\n    Method called before evaluation. Can be used to perform any setup tasks.\n\n    Args:\n        test (Test): The test case.\n        trace (Trace): Captures steps during evaluation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/target_response/","title":"TargetResponse","text":""},{"location":"reference/target_response/#src.agenteval.targets.target_response.TargetResponse","title":"<code>TargetResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A target's response.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>str</code> <p>The response string.</p> <code>data</code> <code>Optional[dict]</code> <p>Additional data (if applicable).</p> Source code in <code>src/agenteval/targets/target_response.py</code> <pre><code>class TargetResponse(BaseModel):\n    \"\"\"A target's response.\n\n    Attributes:\n        response: The response string.\n        data: Additional data (if applicable).\n    \"\"\"\n\n    response: str\n    data: Optional[dict] = None\n</code></pre>"},{"location":"reference/test/","title":"Test","text":""},{"location":"reference/test/#src.agenteval.test.test.Test","title":"<code>Test</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A test case.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the test.</p> <code>steps</code> <code>list[str]</code> <p>List of step to perform for the test.</p> <code>expected_results</code> <code>list[str]</code> <p>List of expected results for the test.</p> <code>initial_prompt</code> <code>Optional[str]</code> <p>The initial prompt.</p> <code>max_turns</code> <code>int</code> <p>Maximum number of turns allowed for the test.</p> <code>hook</code> <code>Optional[str]</code> <p>The module path to an evaluation hook.</p> Source code in <code>src/agenteval/test/test.py</code> <pre><code>class Test(BaseModel, validate_assignment=True):\n    \"\"\"A test case.\n\n    Attributes:\n        name: Name of the test.\n        steps: List of step to perform for the test.\n        expected_results: List of expected results for the test.\n        initial_prompt: The initial prompt.\n        max_turns: Maximum number of turns allowed for the test.\n        hook: The module path to an evaluation hook.\n    \"\"\"\n\n    # do not collect as a pytest\n    __test__ = False\n\n    name: str\n    steps: list[str]\n    expected_results: list[str]\n    initial_prompt: Optional[str] = None\n    max_turns: int\n    hook: Optional[str] = None\n</code></pre>"},{"location":"reference/test_result/","title":"TestResult","text":""},{"location":"reference/test_result/#src.agenteval.test.test_result.TestResult","title":"<code>TestResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The test result.</p> <p>Attributes:</p> Name Type Description <code>test_name</code> <code>str</code> <p>Name of the test.</p> <code>result</code> <code>str</code> <p>Description of the test result.</p> <code>reasoning</code> <code>str</code> <p>The rationale for the test result.</p> <code>passed</code> <code>bool</code> <p><code>True</code> if the test passed, otherwise <code>False</code>.</p> <code>conversation</code> <code>Conversation</code> <p>Captures the interaction between a user and an agent.</p> Source code in <code>src/agenteval/test/test_result.py</code> <pre><code>class TestResult(BaseModel, arbitrary_types_allowed=True):\n    \"\"\"The test result.\n\n    Attributes:\n        test_name: Name of the test.\n        result: Description of the test result.\n        reasoning: The rationale for the test result.\n        passed: `True` if the test passed, otherwise `False`.\n        conversation: Captures the interaction between a user and an agent.\n    \"\"\"\n\n    # do not collect as a pytest\n    __test__ = False\n\n    test_name: str\n    result: str\n    reasoning: str\n    passed: bool\n    conversation: Conversation\n</code></pre>"},{"location":"reference/trace/","title":"Trace","text":""},{"location":"reference/trace/#src.agenteval.trace.Trace","title":"<code>Trace</code>","text":"<p>A context manager which captures steps taken during evaluation.</p> <p>Once the context manager exits, the trace is dumped to a JSON file.</p> <p>Attributes:</p> Name Type Description <code>test_name</code> <code>str</code> <p>Name of the test.</p> <code>trace_dir</code> <code>str</code> <p>Directory to store the trace.</p> <code>start_time</code> <code>datetime</code> <p>Start time of the trace.</p> <code>end_time</code> <code>datetime</code> <p>End time of the trace.</p> <code>steps</code> <code>list</code> <p>List of steps in the trace.</p> Source code in <code>src/agenteval/trace.py</code> <pre><code>class Trace:\n    \"\"\"A context manager which captures steps taken during evaluation.\n\n    Once the context manager exits, the trace is dumped to a JSON file.\n\n    Attributes:\n        test_name (str): Name of the test.\n        trace_dir (str): Directory to store the trace.\n        start_time (datetime): Start time of the trace.\n        end_time (datetime): End time of the trace.\n        steps (list): List of steps in the trace.\n\n    \"\"\"\n\n    def __init__(self, test_name: str, work_dir: str):\n        \"\"\"\n        Initialize the trace handler.\n\n        Args:\n            test_name (str): Name of the test.\n            work_dir (str): Directory to store the trace.\n        \"\"\"\n        self.test_name = test_name\n        self.trace_dir = os.path.join(work_dir, _TRACE_DIR)\n        self.start_time = None\n        self.end_time = None\n        self.steps = []\n\n    def __enter__(self):\n        self.start_time = datetime.now(timezone.utc)\n        return self\n\n    def __exit__(self, *exc):\n        self.end_time = datetime.now(timezone.utc)\n        self._dump_trace()\n\n    def _dump_trace(self):\n        os.makedirs(self.trace_dir, exist_ok=True)\n\n        with open(os.path.join(self.trace_dir, f\"{self.test_name}.json\"), \"w\") as f:\n            json.dump(self._get_trace(), f, default=str)\n\n    def _get_trace(self) -&gt; str:\n        return {\n            \"test_name\": self.test_name,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"steps\": self.steps,\n        }\n\n    def add_step(self, step_name: Optional[str] = None, **kwargs):\n        \"\"\"Add a step to the trace.\n\n        Args:\n            step_name (Optional[str]): The name of the step. Defaults to\n                the name of the caller function\n        \"\"\"\n        step_name = step_name or inspect.stack()[1].function\n        step = {\"timestamp\": datetime.now(timezone.utc), \"step_name\": step_name}\n        step.update(kwargs)\n        self.steps.append(step)\n</code></pre>"},{"location":"reference/trace/#src.agenteval.trace.Trace.__init__","title":"<code>__init__(test_name, work_dir)</code>","text":"<p>Initialize the trace handler.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str</code> <p>Name of the test.</p> required <code>work_dir</code> <code>str</code> <p>Directory to store the trace.</p> required Source code in <code>src/agenteval/trace.py</code> <pre><code>def __init__(self, test_name: str, work_dir: str):\n    \"\"\"\n    Initialize the trace handler.\n\n    Args:\n        test_name (str): Name of the test.\n        work_dir (str): Directory to store the trace.\n    \"\"\"\n    self.test_name = test_name\n    self.trace_dir = os.path.join(work_dir, _TRACE_DIR)\n    self.start_time = None\n    self.end_time = None\n    self.steps = []\n</code></pre>"},{"location":"reference/trace/#src.agenteval.trace.Trace.add_step","title":"<code>add_step(step_name=None, **kwargs)</code>","text":"<p>Add a step to the trace.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>Optional[str]</code> <p>The name of the step. Defaults to the name of the caller function</p> <code>None</code> Source code in <code>src/agenteval/trace.py</code> <pre><code>def add_step(self, step_name: Optional[str] = None, **kwargs):\n    \"\"\"Add a step to the trace.\n\n    Args:\n        step_name (Optional[str]): The name of the step. Defaults to\n            the name of the caller function\n    \"\"\"\n    step_name = step_name or inspect.stack()[1].function\n    step = {\"timestamp\": datetime.now(timezone.utc), \"step_name\": step_name}\n    step.update(kwargs)\n    self.steps.append(step)\n</code></pre>"},{"location":"targets/","title":"Targets","text":"<p>A target represents the agent you want to test.</p>"},{"location":"targets/#base-configurations","title":"Base configurations","text":"<p>Info</p> <p>This project uses Boto3's credential resolution chain to determine the AWS credentials to use. Please refer to the Boto3 documentation for more details.</p> agenteval.yml<pre><code>target:\n  aws_profile: my-profile\n  aws_region: us-west-2\n  endpoint_url: my-endpoint-url\n  max_retry: 10\n</code></pre> <p><code>aws_profile</code> (string; optional)</p> <p>A profile name that is used to create a Boto3 session.</p> <p><code>aws_region</code> (string; optional)</p> <p>The AWS region that is used to create a Boto3 session.</p> <p><code>endpoint_url</code> (string; optional)</p> <p>The endpoint URL for the AWS service which is used to construct the Boto3 client.</p> <p><code>max_retry</code> (integer; optional)</p> <p>Configures the Boto3 client with the maximum number of retry attempts allowed. The default is <code>10</code>.</p>"},{"location":"targets/#built-in-targets","title":"Built-in targets","text":"<ul> <li>Agents for Amazon Bedrock</li> <li>Amazon Bedrock Flows</li> <li>Knowledge bases for Amazon Bedrock</li> <li>Amazon Q for Business</li> <li>Amazon SageMaker endpoints</li> <li>Amazon Lex-v2</li> </ul>"},{"location":"targets/bedrock_agents/","title":"Agents for Amazon Bedrock","text":"<p>Agents for Amazon Bedrock offers you the ability to build and configure autonomous agents in your application. For more information, visit the AWS documentation here.</p>"},{"location":"targets/bedrock_agents/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>InvokeAgent</li> </ul>"},{"location":"targets/bedrock_agents/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: bedrock-agent\n  bedrock_agent_id: my-agent-id\n  bedrock_agent_alias_id: my-alias-id\n  bedrock_session_attributes:\n    first_name: user-name\n  bedrock_prompt_session_attributes:\n    timezone: user-timezone\n</code></pre> <p><code>bedrock_agent_id</code> (string)</p> <p>The unique identifier of the Bedrock agent.</p> <p><code>bedrock_agent_alias_id</code> (string)</p> <p>The alias of the Bedrock agent.</p> <p><code>bedrock_session_attributes</code> (map; optional)</p> <p>The attributes that persist over a session between a user and agent, with the same sessionId belong to the same session, as long as the session time limit (the idleSessionTTLinSeconds) has not been surpassed. For example:</p> <pre><code>bedrock_session_attributes:\n  first_name: user-name\n</code></pre> <p><code>bedrock_prompt_session_attributes</code> (map; optional)</p> <p>The attributes that persist over a single call of InvokeAgent. For example:</p> <pre><code>bedrock_prompt_session_attributes:\n    timezone: user-timezone\n</code></pre>"},{"location":"targets/bedrock_flows/","title":"Amazon Bedrock Flows","text":"<p>Amazon Bedrock Flows offer the ability to link prompts, foundation models, and other AWS services into end-to-end workflows through a graphical UI. For more information, visit the AWS documentation here.</p>"},{"location":"targets/bedrock_flows/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>InvokeFlow</li> </ul>"},{"location":"targets/bedrock_flows/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: bedrock-flow\n  bedrock_flow_id: my-flow-id\n  bedrock_flow_alias_id: my-alias-id\n</code></pre> <p><code>bedrock_flow_id</code> (string)</p> <p>The unique identifier of the Bedrock flow. Typically 10 characters uppercase alphanumeric.</p> <p><code>bedrock_flow_alias_id</code> (string)</p> <p>The alias of the Bedrock flow. Typically 10 characters uppercase alphanumeric.</p>"},{"location":"targets/bedrock_knowledge_bases/","title":"Knowledge bases for Amazon Bedrock","text":"<p>Knowledge bases for Amazon Bedrock provides you the capability of amassing data sources into a repository of information. With knowledge bases, you can easily build an application that takes advantage of retrieval augmented generation (RAG), a technique in which the retrieval of information from data sources augments the generation of model responses. For more information, visit the AWS documentation here.</p>"},{"location":"targets/bedrock_knowledge_bases/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>RetrieveAndGenerate</li> </ul>"},{"location":"targets/bedrock_knowledge_bases/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: bedrock-knowledge-base\n  model_id: my-model-id\n  knowledge_base_id: my-kb-id\n</code></pre> <p><code>model_id</code> (string)</p> <p>The unique identifier of the foundation model used to generate a response.</p> <p><code>knowledge_base_id</code> (string)</p> <p>The unique identifier of the knowledge base that is queried and the foundation model used for generation.</p>"},{"location":"targets/custom_targets/","title":"Custom Targets","text":"<p>If you want to test an agent that is not natively supported, you can bring your own Target by defining a Python module containing a subclass of BaseTarget. The name of this module must contain the suffix <code>_target</code> (e.g. <code>my_custom_target</code>).</p> <p>The subclass should implement the <code>invoke</code> method to invoke your agent and return a TargetResponse.</p> my_custom_target.py<pre><code>from agenteval.targets import BaseTarget, TargetResponse\nfrom my_agent import MyAgent\n\nclass MyCustomTarget(BaseTarget):\n\n    def __init__(self, **kwargs):\n        self.agent = MyAgent(**kwargs)\n\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n\n        response = self.agent.invoke(prompt)\n\n        return TargetResponse(response=response)\n</code></pre> <p>Once you have created your subclass, specify the module path to the Target.</p> agenteval.yml<pre><code>target:\n  type: path.to.my_custom_target.MyCustomTarget`\n  my_agent_parameter: \"value\" # (1)\n</code></pre> <ol> <li>This will be passed as <code>kwargs</code> when initializing the Target.</li> </ol> <p>Warning</p> <p>During a run, an instance of the Target will be created for each test in the test plan. We recommend avoiding testing Targets that load large models or vector stores into memory, as this can lead to a memory error. Consider deploying your agent and exposing it as a RESTful service.</p>"},{"location":"targets/custom_targets/#examples","title":"Examples","text":""},{"location":"targets/custom_targets/#rest-api","title":"REST API","text":"<p>We will implement a custom Target that invokes an agent exposed as a REST API.</p> <p>my_api_target.py</p> <pre><code>import json\n\nimport requests\n\nfrom agenteval.targets import BaseTarget, TargetResponse\n\n\nclass MyAPITarget(BaseTarget):\n    def __init__(self, **kwargs):\n        self.url = kwargs.get(\"url\")\n\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n        data = {\"message\": prompt}\n\n        response = requests.post(\n            self.url, json=data, headers={\"Content-Type\": \"application/json\"}\n        )\n\n        return TargetResponse(response=json.loads(response.content)[\"agentResponse\"])\n</code></pre> <p>Create a test plan that references <code>MyAPITarget</code>.</p> <p>agenteval.yml</p> <pre><code>evaluator:\n  model: claude-3\ntarget:\n  type: my_api_target.MyAPITarget\n  url: https://api.example.com/invoke\ntests:\n  get_backlog_tickets:\n    steps:\n    - Ask agent how many tickets are left in the backlog\n    expected_results:\n    - Agent responds with 15 tickets\n</code></pre>"},{"location":"targets/custom_targets/#langchain-agent","title":"LangChain agent","text":"<p>We will create a simple LangChain agent which calculates the length of a given piece of text.</p> <p>my_langchain_target.py</p> <pre><code>from langchain_community.llms import Bedrock\nfrom langchain.agents import tool\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_xml_agent\n\nfrom agenteval.targets import BaseTarget, TargetResponse\n\nllm = Bedrock(model_id=\"anthropic.claude-v2:1\")\n\n@tool\ndef calculate_text_length(text: str) -&gt; int:\n    \"\"\"Returns the length of a given text.\"\"\"\n    return len(text)\n\n\ntools = [calculate_text_length]\nprompt = hub.pull(\"hwchase17/xml-agent-convo\")\n\nagent = create_xml_agent(llm, tools, prompt)\n\n\nclass MyLangChainTarget(BaseTarget):\n    def __init__(self, **kwargs):\n        self.agent = AgentExecutor(agent=agent, tools=tools, verbose=False)\n\n    def invoke(self, prompt: str) -&gt; TargetResponse:\n\n        response = self.agent.invoke({\"input\": prompt})[\"output\"]\n\n        return TargetResponse(response=response)\n</code></pre> <p>Create a test plan that references <code>MyLangChainTarget</code>.</p> <p>agenteval.yml</p> <pre><code>evaluator:\n  model: claude-3\ntarget:\n  type: my_langchain_target.MyLangChainTarget\ntests:\n  calculate_text_length:\n    steps:\n    - \"Ask agent to calculate the length of this text: Hello world!\"\n    expected_results:\n    - The agent responds that the length is 12.\n</code></pre>"},{"location":"targets/lex_v2/","title":"Amazon Lex v2","text":"<p>Amazon Lex V2 is an AWS service for building conversational interfaces for applications using voice and text. Amazon Lex V2 provides the deep functionality and flexibility of natural language understanding (NLU) and automatic speech recognition (ASR) so you can build highly engaging user experiences with lifelike, conversational interactions, and create new categories of products. For more information, visit the AWS documentation here.</p>"},{"location":"targets/lex_v2/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>RecognizeText</li> </ul>"},{"location":"targets/lex_v2/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: lex-v2\n  bot_id: my-bot-id\n  bot_alias_id: my-bot-alias-id\n  locale_id: my-locale-id\n</code></pre> <p><code>bot_id</code> (string)</p> <p>The unique identifier of the Amazon Lex v2 chatbot.</p> <p><code>bot_alias_id</code> (string; optional)</p> <p>The alias of the Amazon Lex v2 chatbot.</p> <p><code>locale_id</code> (string; optional)</p> <p>The locale of the Amazon Lex v2 chatbot.</p>"},{"location":"targets/q_business/","title":"Amazon Q Business","text":"<p>Amazon Q Business is a generative AI\u2013powered assistant that can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in your enterprise systems. For more information, visit the AWS documentation here.</p>"},{"location":"targets/q_business/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>ChatSync</li> </ul>"},{"location":"targets/q_business/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: q-business\n  q_business_application_id: my-app-id\n  q_business_user_id: my-user-id\n</code></pre> <p><code>q_business_application_id</code> (string)</p> <p>The unique identifier of the Amazon Q application.</p> <p><code>q_business_user_id</code> (string; optional)</p> <p>The identifier of the Amazon Q user.</p>"},{"location":"targets/sagemaker_endpoints/","title":"Amazon SageMaker endpoints","text":"<p>Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. SageMaker endpoints are used to host ML models for real-time inference use cases. For more information, visit the AWS documentation here.</p> <p>Info</p> <p>The current implementation uses JSON to encode the data sent in requests and decode the data received in responses. Please ensure that your SageMaker endpoint can handle requests and responses with the Content-Type and Accept headers set to <code>application/json</code> before proceeding.</p>"},{"location":"targets/sagemaker_endpoints/#prerequisites","title":"Prerequisites","text":"<p>The principal must have the following permissions:</p> <ul> <li>InvokeEndpoint</li> </ul>"},{"location":"targets/sagemaker_endpoints/#configurations","title":"Configurations","text":"agenteval.yml<pre><code>target:\n  type: sagemaker-endpoint\n  endpoint_name: my-endpoint-name\n  request_body:\n    input_text: None\n    temperature: 0.1\n  input_path: $.input_text\n  output_path: $.[0].generated_text\n  custom_attributes: my-attributes\n  target_model: my-model\n  target_variant: my-variant\n  target_container_hostname: my-hostname\n  inference_component_name: my-component-name\n</code></pre> <p><code>endpoint_name</code> (string)</p> <p>The name of the Amazon SageMaker endpoint.</p> <p><code>request_body</code> (map)</p> <p>The data that is sent to the endpoint, which includes a placeholder for the prompt. During a run, the placeholder will be replaced by a prompt generated by the Evaluator. For example:</p> <pre><code>request_body:\n  input_text: None # prompt\n  temperature: 0.1\n</code></pre> <p><code>input_path</code> (string)</p> <p>A JSONPath expression to match the field for the input prompt in the request body. For the <code>request_body</code> below:</p> <pre><code>request_body:\n  input_text: None # prompt\n  temperature: 0.1\n</code></pre> <p>The <code>input_path</code> would be <code>$.input_text</code>.</p> <p><code>output_path</code> (string)</p> <p>A JSONPath expression to match the generated text in the response body. For example, if the endpoint returns the following:</p> <pre><code>[{ \"generated_text\": \"Hello!\" }]\n</code></pre> <p>The <code>output_path</code> would be <code>$.[0].generated_text</code>.</p> <p><code>custom_attributes</code> (string; optional)</p> <p>Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker endpoint.</p> <p><code>target_model</code> (string; optional)</p> <p>The model to request for inference when invoking a multi-model endpoint.</p> <p><code>target_variant</code> (string; optional)</p> <p>The production variant to send the inference request to when invoking an endpoint that is running two or more variants.</p> <p><code>target_container_hostname</code> (string; optional)</p> <p>The hostname of the container to invoke if the endpoint hosts multiple containers and is configured to use direct invocation.</p> <p><code>inference_component_name</code> (string; optional)</p> <p>The name of the inference component to invoke if the endpoint hosts one or more inference components.</p>"}]}